# Why-So-Deep

<h1 align="center">
  <a href="https://usmanmaqbool.github.io/why-so-deep"><img src="https://usmanmaqbool.github.io/assets/images/maqbool/maqbool.png" alt="why-so-deep" style="height: 164px;"></a>
  
</h1>
<p align="center">Multiple AcuQitation of perceptiBle regiOns for priOr Learning (MAQBOOL)</p>
<p align="center">
  ⭐️ If you like Why-So-Deep, give it a star on GitHub! ⭐️
  <br>
  <a href="https://twitter.com/MUsmanMBhutta"><img src="https://img.shields.io/twitter/follow/MUsmanMBhutta.svg?style=social" alt="Twitter Follow" /></a>
  <a href="#license"><img src="https://img.shields.io/github/license/sourcerer-io/hall-of-fame.svg?colorB=ff0000"></a>
</p>

#### Official implementation:
* [Why-So-Deep](https://usmanmaqbool.github.io/why-so-deep): Towards Boosting Previously Trained Models for Visual Place Recognition (RA-L/ICRA 2022) [[paper]](https://arxiv.org/abs/2201.03212) [[project website](https://usmanmaqbool.github.io/why-so-deep)]

##### Source Code
> Source Code will be uploaded soon, meanwhile you can use our results.

#### Results
If you want to add MAQBOOL results, tested on Pittsburgh and Tokyo247 dataset, in comparison with your work at 4096-D and 512-D. You can use the `dat` files in the `results` folder. 
Names of these `dat` files are explained in our [project page](https://usmanmaqbool.github.io/why-so-deep), so that you can easily use. 
Furthermore, if you need help in plotting the results using Tikz and latex, please follow this [little tutorial](https://usmanmaqbool.github.io/how-to-add-tikz-graphs-in-latex/).
## Citation

If you find this repo useful for your research, please consider citing the paper
```bib
@article{whysodeepBhutta22,
	title={Why-So-Deep: Towards Boosting Previously Trained Models for Visual Place Recognition}, 	
	author={M. Usman Maqbool Bhutta and Yuxiang Sun and Darwin Lau and Ming Liu},	
    year={2022},
	eprint={2201.03212},
	archivePrefix={arXiv},
	primaryClass={cs.CV}
}
```
